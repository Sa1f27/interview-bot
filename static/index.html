<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            margin: 0;
            padding: 0;
            overflow: hidden;
        }

        .interview-container {
            display: flex;
            height: 100vh;
        }

        .video-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            background: rgba(255, 255, 255, 0.95);
            padding: 40px;
        }

        .transcript-panel {
            width: 400px;
            background: white;
            padding: 24px;
            overflow-y: auto;
            border-left: 2px solid #e5e7eb;
        }

        .ai-avatar {
            width: 200px;
            height: 200px;
            position: relative;
            margin-bottom: 40px;
        }

        #statusIndicator {
            position: absolute;
            bottom: 10px;
            right: 10px;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #6c757d;
            border: 4px solid white;
            transition: all 0.3s;
        }

        #statusIndicator.listening {
            background: #dc3545;
            animation: pulse 1.5s infinite;
        }

        #statusIndicator.speaking {
            background: #28a745;
        }

        @keyframes pulse {

            0%,
            100% {
                box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
            }

            50% {
                box-shadow: 0 0 0 15px rgba(220, 53, 69, 0);
            }
        }

        .transcript-entry {
            margin-bottom: 20px;
            padding: 12px;
            border-radius: 8px;
            background: #f8f9fa;
        }

        .transcript-entry.user {
            background: #e3f2fd;
        }

        .transcript-entry .role {
            font-weight: 600;
            margin-bottom: 6px;
            color: #495057;
        }

        .transcript-entry .text {
            color: #212529;
            line-height: 1.5;
        }

        .control-bar {
            padding: 20px;
            text-align: center;
        }

        #startBtn,
        #endBtn {
            padding: 14px 32px;
            font-size: 1.1rem;
            border-radius: 8px;
            border: none;
            cursor: pointer;
            transition: all 0.3s;
        }

        #startBtn {
            background: #0d6efd;
            color: white;
        }

        #startBtn:hover {
            background: #0b5ed7;
        }

        #endBtn {
            background: #dc3545;
            color: white;
        }

        .status-text {
            margin-top: 20px;
            font-size: 1.2rem;
            color: #495057;
            font-weight: 500;
        }

        .hidden {
            display: none !important;
        }
    </style>
</head>

<body>
    <div class="interview-container">
        <div class="video-panel">
            <div class="ai-avatar">
                <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                    <circle cx="50" cy="50" r="48" fill="#0d6efd" />
                    <circle cx="35" cy="40" r="5" fill="white" />
                    <circle cx="65" cy="40" r="5" fill="white" />
                    <path id="mouth" d="M 30 60 Q 50 70 70 60" stroke="white" stroke-width="3" fill="none" />
                </svg>
                <div id="statusIndicator"></div>
            </div>
            <div class="status-text" id="statusText">Ready to start</div>
            <div class="control-bar">
                <button id="startBtn">Start Interview</button>
                <button id="endBtn" class="hidden">End Interview</button>
            </div>
        </div>
        <div class="transcript-panel" id="transcriptPanel">
            <h5 class="mb-3">Interview Transcript</h5>
        </div>
    </div>

    <script>
        const startBtn = document.getElementById('startBtn');
        const endBtn = document.getElementById('endBtn');
        const transcriptPanel = document.getElementById('transcriptPanel');
        const statusIndicator = document.getElementById('statusIndicator');
        const statusText = document.getElementById('statusText');
        const mouth = document.getElementById('mouth');

        let testId = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let audioContext = null;
        let vadNode = null;
        let isRecording = false;
        let currentAudio = null;

        startBtn.addEventListener('click', startInterview);
        endBtn.addEventListener('click', endInterview);

        function setStatus(text, indicator = '') {
            statusText.textContent = text;
            statusIndicator.className = indicator;
        }

        function addTranscript(role, text) {
            const entry = document.createElement('div');
            entry.className = `transcript-entry ${role.toLowerCase()}`;
            entry.innerHTML = `
                <div class="role">${role}</div>
                <div class="text">${text}</div>
            `;
            transcriptPanel.appendChild(entry);
            transcriptPanel.scrollTop = transcriptPanel.scrollHeight;
        }

        async function startInterview() {
            startBtn.classList.add('hidden');
            endBtn.classList.remove('hidden');
            setStatus('Starting interview...', '');

            try {
                const response = await fetch('/start_test', { method: 'POST' });
                const data = await response.json();
                testId = data.test_id;

                addTranscript('AI', data.question);
                playAudio(data.audio_path, () => {
                    startRecording();
                });
            } catch (error) {
                console.error('Start error:', error);
                addTranscript('System', 'Error starting interview');
                resetUI();
            }
        }

        function endInterview() {
            stopRecording();
            setStatus('Interview ended', '');
            resetUI();
        }

        function resetUI() {
            startBtn.classList.remove('hidden');
            endBtn.classList.add('hidden');
            testId = null;
            if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
            }
        }

        function playAudio(audioPath, callback) {
            if (!audioPath) {
                if (callback) callback();
                return;
            }

            setStatus('AI is speaking...', 'speaking');
            mouth.setAttribute('d', 'M 30 60 Q 50 75 70 60');

            currentAudio = new Audio(audioPath);
            currentAudio.onended = () => {
                mouth.setAttribute('d', 'M 30 60 Q 50 70 70 60');
                setStatus('Listening...', '');
                currentAudio = null;
                if (callback) callback();
            };
            currentAudio.onerror = () => {
                console.error('Audio playback error');
                if (callback) callback();
            };
            currentAudio.play();
        }

        async function startRecording() {
            if (isRecording || !testId) return;
            isRecording = true;

            setStatus('Listening to your response...', 'listening');

            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

                audioContext = new AudioContext();
                await audioContext.audioWorklet.addModule('/static/vad-processor.js');

                const microphone = audioContext.createMediaStreamSource(stream);
                vadNode = new AudioWorkletNode(audioContext, 'vad-processor', {
                    processorOptions: {
                        voiceStopDelay: 2000,
                        speakingThreshold: 0.02,
                        minSpeakingDuration: 500
                    }
                });

                vadNode.port.onmessage = (event) => {
                    if (event.data.speaking === false) {
                        stopRecording();
                    }
                };

                microphone.connect(vadNode);

                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = () => {
                    stream.getTracks().forEach(track => track.stop());
                    sendAudioAndGetResponse();
                };

                mediaRecorder.start();
            } catch (error) {
                console.error('Recording error:', error);
                setStatus('Microphone error', '');
                isRecording = false;
            }
        }

        function stopRecording() {
            if (!isRecording) return;
            isRecording = false;

            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
            if (vadNode) {
                vadNode.disconnect();
                vadNode = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            setStatus('Processing...', '');
        }

        async function sendAudioAndGetResponse() {
            if (audioChunks.length === 0) {
                console.log('No audio recorded');
                startRecording();
                return;
            }

            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
            audioChunks = [];

            try {
                const response = await fetch(`/interview/${testId}`, {
                    method: 'POST',
                    body: audioBlob
                });

                if (!response.ok) {
                    throw new Error(`Server error: ${response.status}`);
                }

                const data = await response.json();

                if (data.user_transcript) {
                    addTranscript('You', data.user_transcript);
                }

                addTranscript('AI', data.text);

                if (data.ended) {
                    playAudio(data.audio_path, () => {
                        endInterview();
                    });
                } else {
                    playAudio(data.audio_path, () => {
                        startRecording();
                    });
                }
            } catch (error) {
                console.error('Interview step error:', error);
                addTranscript('System', 'Error occurred. Please try again.');
                startRecording();
            }
        }
    </script>
</body>

</html>