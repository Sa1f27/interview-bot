<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            background-color: #f0f2f5;
            color: #333;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        }
        .interview-container {
            display: flex;
            flex-direction: column;
            height: 100vh;
        }
        .main-content {
            flex-grow: 1;
            display: flex;
        }
        .video-panel {
            flex-grow: 1;
            background-color: #fff;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            border-right: 1px solid #d1d5db;
        }
        .transcript-panel {
            width: 380px;
            background-color: #f9fafb;
            padding: 20px;
            overflow-y: auto;
            height: calc(100vh - 80px);
            border-left: 1px solid #d1d5db;
        }
        .control-bar {
            height: 80px;
            background-color: #fff;
            display: flex;
            align-items: center;
            justify-content: center;
            border-top: 1px solid #d1d5db;
        }
        .transcript-entry {
            margin-bottom: 15px;
        }
        .transcript-entry .role {
            font-weight: 600;
            margin-bottom: 4px;
        }
        .transcript-entry .text {
            white-space: pre-wrap;
            color: #555;
        }
        .ai-avatar {
            width: 180px;
            height: 180px;
            position: relative;
        }
        #statusIndicator {
            position: absolute;
            bottom: 15px;
            right: 15px;
            width: 25px;
            height: 25px;
            border-radius: 50%;
            background-color: #6c757d;
            border: 3px solid #fff;
        }
        #statusIndicator.listening {
            background-color: #dc3545;
            animation: pulse 1.5s infinite;
        }
        #statusIndicator.speaking {
            background-color: #198754;
        }
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7); }
            70% { box-shadow: 0 0 0 12px rgba(220, 53, 69, 0); }
            100% { box-shadow: 0 0 0 0 rgba(220, 53, 69, 0); }
        }
        #startBtn, #endBtn {
            font-size: 1.1rem;
            padding: 12px 24px;
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <div class="interview-container">
        <div class="main-content">
            <div class="video-panel">
                <div class="ai-avatar">
                    <svg id="ai-avatar-svg" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                        <circle cx="50" cy="50" r="48" fill="#0d6efd"/>
                        <path id="mouth" d="M 30 60 Q 50 70 70 60" stroke="white" stroke-width="3" fill="none" />
                    </svg>
                    <div id="statusIndicator" class="status-indicator"></div>
                </div>
            </div>
            <div class="transcript-panel" id="transcriptPanel">
                <!-- Transcript entries will be added here -->
            </div>
        </div>
        <div class="control-bar">
            <button id="startBtn" class="btn btn-primary">Start Interview</button>
            <button id="endBtn" class="btn btn-danger d-none">End Interview</button>
        </div>
    </div>

    <script>
        const startBtn = document.getElementById('startBtn');
        const endBtn = document.getElementById('endBtn');
        const transcriptPanel = document.getElementById('transcriptPanel');
        const statusIndicator = document.getElementById('statusIndicator');
        const mouth = document.getElementById('mouth');

        let testId = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let audioContext;
        let vadNode;

        startBtn.addEventListener('click', startInterview);

        function addTranscriptEntry(role, text) {
            const entry = document.createElement('div');
            entry.classList.add('transcript-entry');
            const roleEl = document.createElement('div');
            roleEl.classList.add('role');
            roleEl.textContent = role;
            const textEl = document.createElement('div');
            textEl.classList.add('text');
            textEl.textContent = text;
            entry.appendChild(roleEl);
            entry.appendChild(textEl);
            transcriptPanel.appendChild(entry);
            transcriptPanel.scrollTop = transcriptPanel.scrollHeight;
        }

        async function startInterview() {
            startBtn.classList.add('d-none');
            endBtn.addEventListener('click', endInterview, { once: true });
            endBtn.classList.remove('d-none');
            addTranscriptEntry('System', 'Starting interview...');

            try {
                const response = await fetch('/start_test', { method: 'POST' });
                const data = await response.json();
                testId = data.test_id;

                addTranscriptEntry('AI', data.question);
                playAudio(data.audio_path, () => {
                    startRecording();
                });

            } catch (error) {
                console.error('Error starting interview:', error);
                addTranscriptEntry('System', 'Error starting interview.');
            }
        }

        function endInterview() {
            stopRecording();
            addTranscriptEntry('System', 'Interview ended.');
            startBtn.classList.remove('d-none');
            endBtn.classList.add('d-none');
            statusIndicator.className = 'status-indicator';
            testId = null; // Reset testId
        }

        async function sendAudioAndGetResponse() {
            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
            if (audioBlob.size === 0) {
                console.log("No audio recorded, restarting listening.");
                startRecording(); // If no audio, just start listening again
                return;
            }

            try {
                const response = await fetch(`/interview/${testId}`, {
                    method: 'POST',
                    body: audioBlob
                });

                if (!response.ok) {
                    throw new Error(`Server responded with ${response.status}`);
                }

                const data = await response.json();
                addTranscriptEntry('AI', data.text);
                playAudio(data.audio_path, () => {
                    data.ended ? endInterview() : startRecording();
                });
            } catch (error) {
                console.error('Error during interview step:', error);
                addTranscriptEntry('System', 'An error occurred. Please try again.');
            }
        }

        function playAudio(audioPath, onEndedCallback) {
            if (!audioPath) {
                if (onEndedCallback) onEndedCallback();
                return;
            }
            const audio = new Audio(audioPath);
            audio.onended = () => {
                statusIndicator.classList.remove('speaking');
                animateMouth(false);
                if (onEndedCallback) onEndedCallback();
            };
            audio.play();
            animateMouth(true);
        }

        async function startRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') return;
            if (!testId) return; // Don't start if interview has ended

            statusIndicator.classList.remove('speaking');
            statusIndicator.classList.add('listening');
            addTranscriptEntry('System', 'Listening...');

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            audioContext = new AudioContext();
            await audioContext.audioWorklet.addModule('/static/vad-processor.js');
            const microphone = audioContext.createMediaStreamSource(stream);
            vadNode = new AudioWorkletNode(audioContext, 'vad-processor', {
                processorOptions: {
                    voiceStopDelay: 1500, // Increased delay to 1.5 seconds
                    speakingThreshold: 0.05
                }
            });

            vadNode.port.onmessage = (event) => {
                if (event.data.speaking === false) {
                    stopRecording();
                }
            };

            microphone.connect(vadNode);

            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];

            mediaRecorder.ondataavailable = (event) => {
                audioChunks.push(event.data);
            };

            mediaRecorder.onstop = () => {
                sendAudioAndGetResponse();
            };

            mediaRecorder.start();
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
            if (vadNode) {
                vadNode.disconnect();
                vadNode = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            statusIndicator.classList.remove('listening');
            addTranscriptEntry('System', 'Processing...');
        }

        function animateMouth(speaking) {
            if (speaking) {
                mouth.setAttribute('d', 'M 30 60 Q 50 80 70 60');
            } else {
                mouth.setAttribute('d', 'M 30 60 Q 50 70 70 60');
            }
        }

    </script>
</body>
</html>